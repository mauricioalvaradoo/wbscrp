{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping en el Journal of Macroeconomics\n",
    "\n",
    "El objetivo es vincularme con la página web del Journal of Macroeconomics para extraer la informacion de cada volumen, los articulos de cada uno, los autores, los links, entre otros.\n",
    "\n",
    "Voy a trabajar con un entorno virtual llamado `env`. El Driver de Chrome se puede descargar en https://sites.google.com/chromium.org/driver/downloads\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep\n",
    "# from numba import njit\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos vinculamos con la sección de articulos dentro del sitio web del Journal of Macroeconomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_import(url):\n",
    "    driver = webdriver.Chrome(\"./driver/chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # Clicks a los botones\n",
    "    for i in range(1, 30):\n",
    "        try:\n",
    "            button = driver.find_element(By.XPATH, value=f'/html/body/div[3]/div/div/div/main/div[2]/div/section[2]/div/div/ol/li[{i}]/button')\n",
    "            button.click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Código fuente como HTML\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los links para la importacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13756\\2084784881.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"./driver/chromedriver.exe\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 234 ms\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "url_1 = \"https://www.sciencedirect.com/journal/journal-of-macroeconomics/issues?page=1\"\n",
    "url_2 = \"https://www.sciencedirect.com/journal/journal-of-macroeconomics/issues?page=2\"\n",
    "url_3 = \"https://www.sciencedirect.com/journal/journal-of-macroeconomics/issues?page=3\"\n",
    "\n",
    "page_1 = html_import(url_1)\n",
    "page_2 = html_import(url_2)\n",
    "page_3 = html_import(url_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras haber importado todos los elementos dentro de los links, es importante definir los elementos que deseamos importar y sus clases\n",
    "\n",
    "* Secciones: `<li class=\"accordion-panel js-accordion-panel\">`\n",
    "* Volumenes: `<div class=\"issue-item u-margin-s-bottom\">`\n",
    "* Nombre del volumen: `<span class=\"anchor-text\">`\n",
    "* Link: `<a class=\"anchor js-issue-item-link text-m anchor-default\">`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volumens(soup):\n",
    "    sections = soup.find_all(\"li\", {\"class\": \"accordion-panel js-accordion-panel\"})\n",
    "\n",
    "    list_names=[]\n",
    "    list_urls=[]\n",
    "    list_date=[]\n",
    "\n",
    "    for section in sections:\n",
    "        volumens = section.find_all(\"div\", {\"class\": \"issue-item u-margin-s-bottom\"})\n",
    "\n",
    "        for volume in volumens:\n",
    "            name = volume.find(\"span\", {\"class\": \"anchor-text\"}).text\n",
    "            url = volume.find(\"a\", {\"class\": \"anchor js-issue-item-link text-m anchor-default\"}).get(\"href\")\n",
    "            date = volume.find(\"h3\", {\"class\": \"js-issue-status text-s\"}).text\n",
    "\n",
    "            # Guardando los resultados\n",
    "            list_names.append(name)\n",
    "            list_urls.append(url)\n",
    "            list_date.append(date)\n",
    "\n",
    "    return list_names, list_urls, list_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume_name</th>\n",
       "      <th>volumen_date</th>\n",
       "      <th>volume_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Volume 74</td>\n",
       "      <td>December 2022</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/74/suppl/C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volume 73</td>\n",
       "      <td>September 2022</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/73/suppl/C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Volume 72</td>\n",
       "      <td>June 2022</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/72/suppl/C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Volume 71</td>\n",
       "      <td>March 2022</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/71/suppl/C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Volume 70</td>\n",
       "      <td>December 2021</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/70/suppl/C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Volume 2, Issue 1</td>\n",
       "      <td>Pages 1-102 (Winter 1980)</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/2/issue/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Volume 1, Issue 4</td>\n",
       "      <td>Pages 321-426 (Autumn 1979)</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/1/issue/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Volume 1, Issue 3</td>\n",
       "      <td>Pages 245-320 (Summer 1979)</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/1/issue/3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Volume 1, Issue 2</td>\n",
       "      <td>Pages 149-243 (Spring 1979)</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/1/issue/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Volume 1, Issue 1</td>\n",
       "      <td>Pages 1-147 (Winter 1979)</td>\n",
       "      <td>/journal/journal-of-macroeconomics/vol/1/issue/1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            volume_name                 volumen_date  \\\n",
       "0            Volume 74                 December 2022   \n",
       "1            Volume 73                September 2022   \n",
       "2            Volume 72                     June 2022   \n",
       "3            Volume 71                    March 2022   \n",
       "4            Volume 70                 December 2021   \n",
       "..                  ...                          ...   \n",
       "175  Volume 2, Issue 1     Pages 1-102 (Winter 1980)   \n",
       "176  Volume 1, Issue 4   Pages 321-426 (Autumn 1979)   \n",
       "177  Volume 1, Issue 3   Pages 245-320 (Summer 1979)   \n",
       "178  Volume 1, Issue 2   Pages 149-243 (Spring 1979)   \n",
       "179  Volume 1, Issue 1     Pages 1-147 (Winter 1979)   \n",
       "\n",
       "                                            volume_url  \n",
       "0    /journal/journal-of-macroeconomics/vol/74/suppl/C  \n",
       "1    /journal/journal-of-macroeconomics/vol/73/suppl/C  \n",
       "2    /journal/journal-of-macroeconomics/vol/72/suppl/C  \n",
       "3    /journal/journal-of-macroeconomics/vol/71/suppl/C  \n",
       "4    /journal/journal-of-macroeconomics/vol/70/suppl/C  \n",
       "..                                                 ...  \n",
       "175   /journal/journal-of-macroeconomics/vol/2/issue/1  \n",
       "176   /journal/journal-of-macroeconomics/vol/1/issue/4  \n",
       "177   /journal/journal-of-macroeconomics/vol/1/issue/3  \n",
       "178   /journal/journal-of-macroeconomics/vol/1/issue/2  \n",
       "179   /journal/journal-of-macroeconomics/vol/1/issue/1  \n",
       "\n",
       "[180 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_1, urls_1, dates_1 = get_volumens(page_1)\n",
    "names_2, urls_2, dates_2 = get_volumens(page_2)\n",
    "names_3, urls_3, dates_3 = get_volumens(page_3)\n",
    "\n",
    "names = names_1 + names_2 + names_3\n",
    "urls = urls_1 + urls_2 + urls_3\n",
    "dates = dates_1 + dates_2 + dates_3\n",
    "\n",
    "# Dataframe\n",
    "dta_volumens = pd.DataFrame({\"volume_name\": names, \"volumen_date\": dates, \"volume_url\": urls})\n",
    "dta_volumens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Article: `<h3 class=\"text-m u-font-serif u-display-inline\">`\n",
    "* Url: `<a class=\"anchor article-content-title u-margin-xs-top u-margin-s-bottom anchor-default\">`\n",
    "* Name: `<span class=\"js-article-title\">`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(array):\n",
    "    \n",
    "    list_articles = []\n",
    "\n",
    "    for i in array:\n",
    "        # Extraendo los nombres de los articulos en cada HTML\n",
    "        soup = html_import(f\"https://www.sciencedirect.com{i}\")\n",
    "        articles = soup.find_all(\"h3\", {\"class\": \"text-m u-font-serif u-display-inline\"})\n",
    "\n",
    "        for article in articles:\n",
    "            name = article.find(\"span\", {\"class\": \"js-article-title\"}).text\n",
    "            url = article.find(\"a\", {\"class\": \"anchor article-content-title u-margin-xs-top u-margin-s-bottom anchor-default\"}).get(\"href\")\n",
    "            \n",
    "            # Guardando resultados\n",
    "            list_articles.append([i, name, url])    \n",
    "\n",
    "    return list_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "urls = dta_volumens[\"volume_url\"]\n",
    "\n",
    "articles = get_articles(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_articles = pd.DataFrame(articles, columns=[\"volume_url\", \"article_name\", \"article_url\"])\n",
    "dta_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Autores: `<a class=\"author size-m workspace-trigger\">`\n",
    "* Nombre: `<span class=\"text given-name\">`\n",
    "* Apellido: `<span class=\"text surname\">`\n",
    "* Doi: `<a class=\"doi\">`\n",
    "* Keyword: `<div class=\"keyword\">`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_components(array):\n",
    "    \n",
    "    list_components = []\n",
    "\n",
    "    for i in array:\n",
    "        # Entraendo los componentes de cada uno de los articulos\n",
    "        soup = html_import(f\"https://www.sciencedirect.com{i}\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Elementos\n",
    "            doi = soup.find(\"a\", {\"class\": \"doi\"}).get(\"href\")\n",
    "            keywords = soup.find_all(\"div\", {\"class\": \"keyword\"})\n",
    "            group_authors = soup.find_all(\"a\", {\"class\": \"author size-m workspace-trigger\"})\n",
    "\n",
    "            list_authors = []\n",
    "\n",
    "            for authors in group_authors:\n",
    "                name = authors.find(\"span\", {\"class\": \"text given-name\"}).text\n",
    "                surname = authors.find(\"span\", {\"class\": \"text surname\"}).text\n",
    "\n",
    "                author = f\"[{surname}, {name}]\"\n",
    "                list_authors.append(author)\n",
    "\n",
    "            # Union\n",
    "            list_components.append([i, list_authors, doi, keywords])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return list_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "urls = dta_articles[\"article_url\"]\n",
    "\n",
    "components = get_components(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_components = pd.DataFrame(components, columns=[\"article_url\", \"authors\", \"doi\", \"keywords\"])\n",
    "dta_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se puede unir todos los resultados en un único Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = dta_components.merge(dta_articles, how=\"inner\").merge(dta_volumens, how=\"inner\")\n",
    "\n",
    "# Completando el nombre de los urls\n",
    "dta[\"article_url\"] = \"https://www.sciencedirect.com\" + dta[\"article_url\"].astype(str)\n",
    "dta[\"volume_url\"] = \"https://www.sciencedirect.com\" + dta[\"volume_url\"].astype(str)\n",
    "\n",
    "dta_f = dta[[\"article_name\", \"authors\", \"article_url\", \"doi\", \"keywords\", \"volume_name\", \"volumen_date\", \"volume_url\"]]\n",
    "dta_f[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_f.to_csv(\"./Bases de datos/dta_f.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
